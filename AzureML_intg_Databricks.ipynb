{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AzureML_intg_Databricks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX1db7eDMNo9"
      },
      "source": [
        "from azureml.core.compute import DatabricksCompute\n",
        "from azureml.core.databricks import PyPiLibrary\n",
        "from azureml.pipeline.steps import DatabricksStep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyrzrMy6MVSM"
      },
      "source": [
        "#Create an Azure ML workspace object"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gnm-mU0gMud8"
      },
      "source": [
        "ws = Workspace(workspace_name = '<workspace_name>',\n",
        "            subscription_id = '<subscription_ID>',\n",
        "            resource_group = '<resource_group_name>')\n",
        "\n",
        " print('Workspace name: ' + ws.name, \n",
        "     'Azure region: ' + ws.location, \n",
        "     'Subscription id: ' + ws.subscription_id, \n",
        "     'Resource group: ' + ws.resource_group, sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRD1fkFjMvE8"
      },
      "source": [
        "#Create a Databricks compute object."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve3WKeR2MzsS"
      },
      "source": [
        "db_compute_name = os.getenv(\"DATABRICKS_COMPUTE_NAME\", '<name>') # Databricks compute name\n",
        "db_resource_group = os.getenv(\"DATABRICKS_RESOURCE_GROUP\", \"<resource_group_name>\") # Databricks resource group\n",
        "db_workspace_name = os.getenv(\"DATABRICKS_WORKSPACE_NAME\", \"<workspace_name>\") # Databricks workspace name\n",
        "db_access_token = os.getenv(\"DATABRICKS_ACCESS_TOKEN\", \"<access_token>\") # Databricks access token\n",
        "\n",
        " try:\n",
        "     databricks_compute = ComputeTarget(\n",
        "         workspace = ws, name = db_compute_name)\n",
        "     print('Compute target already exists')\n",
        " except ComputeTargetException:\n",
        "     print('Compute not found, will use below parameters to attach new one')\n",
        "     print('db_compute_name {}'.format(db_compute_name))\n",
        "     print('db_resource_group {}'.format(db_resource_group))\n",
        "     print('db_workspace_name {}'.format(db_workspace_name))\n",
        "     print('db_access_token {}'.format(db_access_token))\n",
        "\n",
        "     config = DatabricksCompute.attach_configuration(\n",
        "        resource_group = db_resource_group,\n",
        "        workspace_name = db_workspace_name,\n",
        "        access_token = db_access_token\n",
        "     )\n",
        "     databricks_compute = ComputeTarget.attach(workspace = ws, name = db_compute_name, attach_configuration = config)\n",
        "     databricks_compute.wait_for_completion(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmIJkbMSM4GS"
      },
      "source": [
        "#Create a datastore connection"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aduVahtKNG-G"
      },
      "source": [
        "blob_datastore_name = '<datastore_name>' # Name of the datastore to workspace\n",
        " container_name = os.getenv(\"BLOB_CONTAINER\", \"<container_name>\") # Name of Azure blob container\n",
        " account_name = os.getenv(\"BLOB_ACCOUNTNAME\", \"<account_name>\") # Storage account name\n",
        " account_key = os.getenv(\"BLOB_ACCOUNT_KEY\", \"<account_key>\") # Storage account key\n",
        "\n",
        " blob_datastore = Datastore.register_azure_blob_container(workspace = ws, \n",
        "                                                         datastore_name = blob_datastore_name, \n",
        "                                                         container_name = container_name, \n",
        "                                                         account_name = account_name,\n",
        "                                                         account_key = account_key)\n",
        "\n",
        " datastore = Datastore.get(ws, datastore_name = \"<datastore_name>\")\n",
        " print(datastore.datastore_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRxog2-TNQ8q"
      },
      "source": [
        "#Building the pipeline"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6CxGR73NWDC"
      },
      "source": [
        "input_data = DataReference(datastore = blob_datastore,\n",
        "                          data_reference_name = 'input_data',\n",
        "                          path_on_datastore = 'adventureworks/raw_data'\n",
        "                          )\n",
        "\n",
        "output_data1 = PipelineData(\n",
        "    \"cleaned_data\",\n",
        "    datastore = blob_datastore)\n",
        "\n",
        "output_data2 = PipelineData(\n",
        "    \"linear_regression_model\",\n",
        "    datastore = blob_datastore)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fixZDGbNYz0"
      },
      "source": [
        "dbutils.widgets.get(\"input_data\")\n",
        "i = getArgument(\"input_data\")\n",
        "print (i)\n",
        "\n",
        "dbutils.widgets.get(\"cleaned_data\")\n",
        "o = getArgument(\"cleaned_data\")\n",
        "print (o)\n",
        "\n",
        "customers_input = os.path.join(i, 'AdvWorksCusts.csv')\n",
        "spending_input = os.path.join(i, 'AW_AveMonthSpend.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n2-eMFuNiIa"
      },
      "source": [
        "dbutils.widgets.get(\"input\")\n",
        "i = getArgument(\"input\")\n",
        "\n",
        "dbutils.widgets.get('cleaned_data')\n",
        "o = getArgument('cleaned_data')\n",
        "\n",
        "try: \n",
        "  dbutils.widgets.get(\"input_blob_secretname\") \n",
        "  myinput_blob_secretname = getArgument(\"input_blob_secretname\")\n",
        "\n",
        "  dbutils.widgets.get(\"input_blob_config\")\n",
        "  myinput_blob_config = getArgument(\"input_blob_config\")\n",
        "\n",
        "  dbutils.fs.mount(\n",
        "    source = i,\n",
        "    mount_point = \"/mnt/input\",\n",
        "    extra_configs = {myinput_blob_config:dbutils.secrets.get(scope = \"amlscope\", key = myinput_blob_secretname)})\n",
        "except:\n",
        "  print('datastore already mounted')\n",
        "\n",
        "customers_input = os.path.join(i, 'AdvWorksCusts.csv')\n",
        "spending_input = os.path.join(i, 'AW_AveMonthSpend.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sw5EwHYuNyRx"
      },
      "source": [
        "#Build the Databricks pipeline step"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVtECKI8NqOb"
      },
      "source": [
        "data_prep_path = os.getenv(\"DATABRICKS_PYTHON_SCRIPT_PATH\", \"/Shared/adventure_works_project/data_prep_spark\") # Databricks python script path\n",
        "\n",
        " step1 = DatabricksStep(name = \"data_preparation\",\n",
        "                     run_name = 'data_preparation',\n",
        "                     inputs = [input_data],\n",
        "                     outputs = [output_data1],\n",
        "                     num_workers = 1,\n",
        "                     notebook_path = data_prep_path,\n",
        "                     pypi_libraries = [PyPiLibrary(package = 'scikit-learn')],\n",
        "                     compute_target = databricks_compute,\n",
        "                     allow_reuse = False\n",
        "                     )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSJorEzYNtSr"
      },
      "source": [
        "#Submitting the pipeline to the Azure ML workspace"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCmAFA-nN8md"
      },
      "source": [
        "steps = [step1, step2, step3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqfC1o97OFKU"
      },
      "source": [
        "pipeline = Pipeline(workspace = ws, steps = [steps])\n",
        "    print('Pipeline is built')\n",
        "    pipeline.validate()\n",
        "    print('Pipeline validation complete')\n",
        "\n",
        "    exp_name = 'webinar_run'\n",
        "    exp = Experiment(ws, exp_name)\n",
        "    pipeline_run = exp.submit(pipeline)\n",
        "    print('Pipeline is submitted for execution')\n",
        "\n",
        "    pipeline_run.wait_for_completion(show_output = False) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh0pTmA9OH0e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}